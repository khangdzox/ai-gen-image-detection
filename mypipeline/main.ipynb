{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a593ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "scheduler = DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265606d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = pipeline.vae.cuda()\n",
    "unet = pipeline.unet.cuda()\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open('real_n01514668_18815.JPEG').convert(\"RGB\").resize((512,512))\n",
    "batch = Image.open('ai_008_sdv5_00084.png').convert(\"RGB\").resize((512,512))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9570fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "image_tensor = transform(batch).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vae_output = vae.encode(image_tensor)\n",
    "\n",
    "latents = vae_output.latent_dist.sample() * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ab445",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 30\n",
    "t_start = 20\n",
    "scheduler.set_timesteps(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0273610",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_list = []\n",
    "latent_list = []\n",
    "for t in scheduler.timesteps[t_start:]:\n",
    "    noise = torch.randn_like(latents).cuda()\n",
    "    noise_list.append(noise)\n",
    "    noisy_latents = scheduler.add_noise(latents, noise, t)\n",
    "    # beta_t = scheduler.betas[t].cuda()\n",
    "    # prev_latents = latent_list[-1] if latent_list else latents\n",
    "    # noisy_latents = (1 - beta_t).sqrt() * prev_latents + beta_t.sqrt() * noise\n",
    "    latent_list.append(noisy_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e692c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "denoise_list = []\n",
    "pred_noise_list = []\n",
    "\n",
    "for t, lat in tqdm(zip(scheduler.timesteps[t_start:], latent_list), total=len(latent_list)):\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(lat, t, text_embeddings).sample\n",
    "        pred_noise_list.append(noise_pred)\n",
    "    denoise_lat = scheduler.step(noise_pred, t.item(), lat).prev_sample\n",
    "    denoise_list.append(denoise_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def extract_noise_features(pred_noise: torch.Tensor, noise: torch.Tensor) -> list[torch.Tensor]:\n",
    "    # tensor shape: (batch_size, channels, height, width)\n",
    "    residual = pred_noise - noise\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    for i in range(pred_noise.shape[0]):\n",
    "\n",
    "        pred_mean = pred_noise[i].mean().item()\n",
    "        pred_std = pred_noise[i].std().item()\n",
    "        pred_skew = scipy.stats.skew(pred_noise[i].flatten().cpu().numpy()).item()\n",
    "        pred_kurtosis = scipy.stats.kurtosis(pred_noise[i].flatten().cpu().numpy()).item()\n",
    "        pred_l2 = torch.linalg.norm(pred_noise[i]).item()\n",
    "\n",
    "        pred_fft = torch.fft.fft2(pred_noise[i], norm=\"ortho\")\n",
    "        pred_fft_magnitude = torch.abs(pred_fft).mean().item()\n",
    "        pred_fft_phase = torch.angle(pred_fft).mean().item()\n",
    "\n",
    "        residual_mean = residual[i].mean().item()\n",
    "        residual_std = residual[i].std().item()\n",
    "        residual_skew = scipy.stats.skew(residual[i].flatten().cpu().numpy())\n",
    "        residual_kurtosis = scipy.stats.kurtosis(residual[i].flatten().cpu().numpy())\n",
    "        residual_l2 = torch.norm(residual[i]).item()\n",
    "\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(pred_noise[i].flatten(), noise[i].flatten(), dim=0).item()\n",
    "\n",
    "        batch.append( torch.tensor([\n",
    "            pred_mean, pred_std, pred_skew, pred_kurtosis, pred_l2,\n",
    "            pred_fft_magnitude, pred_fft_phase,\n",
    "            residual_mean, residual_std, residual_skew, residual_kurtosis, residual_l2,\n",
    "            cosine_sim\n",
    "        ]))\n",
    "\n",
    "    return batch\n",
    "\n",
    "def extract_noise_features_no_noise(pred_noise: torch.Tensor) -> list[torch.Tensor]:\n",
    "    # tensor shape: (batch_size, channels, height, width)\n",
    "    batch = []\n",
    "\n",
    "    for i in range(pred_noise.shape[0]):\n",
    "\n",
    "        pred_mean = pred_noise[i].mean().item()\n",
    "        pred_std = pred_noise[i].std().item()\n",
    "        pred_skew = scipy.stats.skew(pred_noise[i].flatten().cpu().numpy()).item()\n",
    "        pred_kurtosis = scipy.stats.kurtosis(pred_noise[i].flatten().cpu().numpy()).item()\n",
    "        pred_l2 = torch.linalg.norm(pred_noise[i]).item()\n",
    "\n",
    "        pred_fft = torch.fft.fft2(pred_noise[i], norm=\"ortho\")\n",
    "        pred_fft_magnitude = torch.abs(pred_fft).mean().item()\n",
    "        pred_fft_phase = torch.angle(pred_fft).mean().item()\n",
    "\n",
    "        batch.append( torch.tensor([\n",
    "            pred_mean, pred_std, pred_skew, pred_kurtosis, pred_l2,\n",
    "            pred_fft_magnitude, pred_fft_phase\n",
    "        ]))\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = []\n",
    "\n",
    "for pred_noise, noise in tqdm(zip(pred_noise_list, noise_list)):\n",
    "    features = extract_noise_features(pred_noise, noise)\n",
    "    extracted_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_list = []\n",
    "\n",
    "for lat in tqdm(denoise_list):\n",
    "    with torch.no_grad():\n",
    "        decode_output = vae.decode(lat / vae.config.scaling_factor).sample\n",
    "        decode_list.append(decode_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17993b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_list = torch.cat(decode_list)\n",
    "decode_tensor = (decode_list.clamp(-1, 1) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_noise(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    return (tensor - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4506940",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(decode_tensor, nrow=5)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_list = [normalize_noise(noise) for noise in noise_list]\n",
    "pred_noise_list = [normalize_noise(noise) for noise in pred_noise_list]\n",
    "noise_diff_list = [noise - pred_noise for noise, pred_noise in zip(noise_list, pred_noise_list)]\n",
    "noise_diff_list = [normalize_noise(noise) for noise in noise_diff_list]\n",
    "\n",
    "latent_diff_list = [latents - lat for lat in latent_list]\n",
    "latent_diff_list = [normalize_noise(noise) for noise in latent_diff_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfddc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(torch.cat(noise_diff_list), nrow=5)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39968055",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(torch.cat(latent_diff_list), nrow=5)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline:\n",
    "\n",
    "    timesteps = 30\n",
    "    t_start = 20\n",
    "\n",
    "    def __init__(self, vae, unet, tokenizer, text_encoder, scheduler, device='cuda'):\n",
    "        self.vae = vae.to(device)\n",
    "        self.unet = unet.to(device)\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "\n",
    "        self.scheduler.set_timesteps(self.timesteps)\n",
    "\n",
    "        text_encoder = text_encoder.to(device)\n",
    "        with torch.no_grad():\n",
    "            self.text_embeddings = text_encoder(tokenizer(\"\", return_tensors=\"pt\").input_ids.to(device))[0]\n",
    "\n",
    "    def __call__(self, batch: torch.Tensor):\n",
    "        batch = batch.to(self.device)\n",
    "\n",
    "        batch_text_embeddings = self.text_embeddings.repeat(batch.shape[0], 1, 1)\n",
    "\n",
    "        # Encode the image using VAE\n",
    "        with torch.no_grad():\n",
    "            vae_output = self.vae.encode(batch)\n",
    "\n",
    "        latents = vae_output.latent_dist.sample() * self.vae.config.scaling_factor\n",
    "\n",
    "        # Add noise to the latents\n",
    "        # noises_list = []\n",
    "        # latents_list = []\n",
    "        noise = torch.randn_like(latents).to(self.device)\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, self.scheduler.timesteps[self.t_start])\n",
    "        # for t in self.scheduler.timesteps[self.t_start:]:\n",
    "        #     noises_list.append(noise)\n",
    "        #     noisy_latents = scheduler.add_noise(latents, noise, t)\n",
    "        #     latents_list.append(noisy_latents)\n",
    "\n",
    "        # denoise_list = []\n",
    "        pred_noises_list = []\n",
    "\n",
    "        for t in tqdm(self.scheduler.timesteps[self.t_start:], desc=\"Denoising\", leave=False):\n",
    "            with torch.no_grad():\n",
    "                noises_pred = self.unet(noisy_latents, t, batch_text_embeddings).sample\n",
    "                pred_noises_list.append(noises_pred)\n",
    "            noisy_latents = self.scheduler.step(noises_pred, t.item(), noisy_latents).prev_sample\n",
    "            # denoise_list.append(denoise_lat)\n",
    "\n",
    "        extracted_features = []\n",
    "        # for pred_noises, noises in tqdm(zip(pred_noises_list, noises_list), total=len(pred_noises_list)):\n",
    "        for pred_noises in tqdm(pred_noises_list, desc=\"Extracting features\", leave=False):\n",
    "            # features = extract_noise_features(pred_noises, noises)\n",
    "            features = extract_noise_features_no_noise(pred_noises)\n",
    "            extracted_features.append(features)\n",
    "\n",
    "        extracted_features = zip(*extracted_features)\n",
    "        extracted_features = [torch.stack(feature) for feature in extracted_features]\n",
    "        extracted_features = torch.stack(extracted_features)\n",
    "\n",
    "        return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub, os, shutil, random\n",
    "\n",
    "datapath = kagglehub.dataset_download(\"yangsangtai/tiny-genimage\")\n",
    "\n",
    "os.makedirs('data/0_real', exist_ok=True)\n",
    "os.makedirs('data/1_fake', exist_ok=True)\n",
    "\n",
    "for dir in os.listdir(datapath):\n",
    "    for file in os.listdir(f'{datapath}/{dir}/val/nature'):\n",
    "        shutil.copy(f'{datapath}/{dir}/val/nature/{file}', f'data/0_real/{dir}_{file}')\n",
    "    for file in os.listdir(f'{datapath}/{dir}/val/ai'):\n",
    "        shutil.copy(f'{datapath}/{dir}/val/ai/{file}', f'data/1_fake/{dir}_{file}')\n",
    "\n",
    "os.makedirs('data_1000/0_real', exist_ok=True)\n",
    "os.makedirs('data_1000/1_fake', exist_ok=True)\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "random_1000_real = random.sample(os.listdir('data/0_real'), 1000)\n",
    "random_1000_fake = random.sample(os.listdir('data/1_fake'), 1000)\n",
    "for file in random_1000_real:\n",
    "    shutil.copy(f'data/0_real/{file}', f'data_1000/0_real/{file}')\n",
    "for file in random_1000_fake:\n",
    "    shutil.copy(f'data/1_fake/{file}', f'data_1000/1_fake/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root='data_1000',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88026325",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_labels = []\n",
    "\n",
    "mypipeline = MyPipeline(\n",
    "    vae=pipeline.vae,\n",
    "    unet=pipeline.unet,\n",
    "    tokenizer=pipeline.tokenizer,\n",
    "    text_encoder=pipeline.text_encoder,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "for batch, labels in tqdm(dataloader):\n",
    "    features = mypipeline(batch)\n",
    "    results.append(features)\n",
    "    results_labels.append(labels)\n",
    "\n",
    "results_ts = torch.cat(results, dim=0)\n",
    "results = results_ts.cpu().numpy()\n",
    "results = results.reshape(results.shape[0], -1)\n",
    "\n",
    "results_labels_ts = torch.cat(results_labels, dim=0)\n",
    "results_labels = results_labels_ts.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE(n_components=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934bd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.fit_transform(results)\n",
    "x_tsne = tsne.fit_transform(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_embedding(X_embedded, labels, title=\"Embedding\"):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels, cmap='coolwarm', alpha=0.6)\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_embedding(x_pca, results_labels, title=\"PCA\")\n",
    "plot_embedding(x_tsne, results_labels, title=\"t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        logits = self.fc(output[:, -1, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "\n",
    "results_mean = results_ts.mean(dim=(0, 1))\n",
    "results_std = results_ts.std(dim=(0, 1))\n",
    "results_ts = (results_ts - results_mean[None, None, :]) / (results_std[None, None, :] + 1e-8)\n",
    "\n",
    "train_indices = torch.randperm(results_ts.shape[0])\n",
    "train_size = int(0.8 * results_ts.shape[0])\n",
    "train_indices, val_indices = train_indices[:train_size], train_indices[train_size:]\n",
    "\n",
    "X_train = results_ts[train_indices]\n",
    "X_test = results_ts[val_indices]\n",
    "y_train = results_labels_ts[train_indices]\n",
    "y_test = results_labels_ts[val_indices]\n",
    "\n",
    "model = LSTMClassifier(input_size=results_ts.shape[-1]).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in tqdm(range(0, X_train.shape[0], batch_size), leave=False):\n",
    "        batch = X_train[i:i+batch_size].to(device)\n",
    "        labels = y_train[i:i+batch_size].to(device)\n",
    "\n",
    "        logits = model(batch)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {running_loss / total:.4f}, Accuracy = {correct / total:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-genereted-image-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
